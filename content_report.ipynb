{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURE HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CONSTANTS\"\"\" \n",
    "\n",
    "CATALOG_ID = \"9f82678130ef47aeb942bf87ab99a3eb\"\n",
    "ORIGIN_TRANSFER_USER = \"truck_transfer\"\n",
    "ORIGIN_URL = \"https://truckcomm.maps.arcgis.com/\"\n",
    "ITEM_ID = \"49396b145216498e99797983b12f45f9\" # INSERT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"IMPORTS\"\"\"\n",
    "\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.gis import Item\n",
    "from arcgis.gis import ContentManager\n",
    "from arcgis import __version__\n",
    "from arcgis.features import FeatureLayerCollection\n",
    "from arcgis.mapping import WebMap\n",
    "from datetime import datetime\n",
    "import ipywidgets as widgets\n",
<<<<<<< HEAD
    "import sys\n",
    "import numpy as np\n",
    "from itertools import islice\n",
=======
    "import re\n",
>>>>>>> main
    "\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE\n",
    "\n",
    "ITEM_COPY_PROPERTIES = ['title', 'type', 'typeKeywords', 'description', 'tags',\n",
    "                        'snippet', 'extent', 'spatialReference', 'name',\n",
    "                        'accessInformation', 'licenseInfo', 'culture', 'url']\n",
    "\n",
    "TEXT_BASED_ITEM_TYPES = frozenset(['Web Map', 'Feature Service', 'Map Service','Web Scene', 'Dashboard',\n",
    "                                   'Image Service', 'Feature Collection', \n",
    "                                   'Feature Collection Template',\n",
    "                                   'Web Mapping Application', 'Mobile Application', \n",
    "                                   'Symbol Set', 'Color Set',\n",
    "                                   'Windows Viewer Configuration'])\n",
    "\n",
    "FILE_BASED_ITEM_TYPES = frozenset(['File Geodatabase','CSV', 'Image', 'KML', 'Locator Package',\n",
    "                                  'Map Document', 'Shapefile', 'Microsoft Word', 'PDF',\n",
    "                                  'Microsoft Powerpoint', 'Microsoft Excel', 'Layer Package',\n",
    "                                  'Mobile Map Package', 'Geoprocessing Package', 'Scene Package',\n",
    "                                  'Tile Package', 'Vector Tile Package'])\n",
    "\n",
    "RELATIONSHIP_TYPES = frozenset(['Map2Service', 'WMA2Code',\n",
    "                                'Map2FeatureCollection', 'MobileApp2Code', 'Service2Data',\n",
    "                                'Service2Service'])\n",
    "\n",
    "_version = [int(i) for i in __version__.split('.')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIGURE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the password for user truck_transfer: ········\n",
      "Connecting ...\n",
      "Connection Successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/arcgis/gis/__init__.py:676: UserWarning: You are logged on as cgs_transfer with an administrator role, proceed with caution.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### HARDCODED CONFIG FOR TESTING SM TRANSFER\n",
    "\n",
    "origin_pass = getpass(prompt=f\"Enter the password for user {ORIGIN_TRANSFER_USER}: \")\n",
    "\n",
    "# Establish origin and target GIS organizations\n",
    "print(\"Connecting ...\")\n",
    "origin = GIS(ORIGIN_URL, ORIGIN_TRANSFER_USER, origin_pass, expiration=9999)\n",
    "print(\"Connection Successful.\")\n",
    "\n",
    "destination = GIS(\"home\", expiration=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = destination.content.get(CATALOG_ID)\n",
    "catalog_table = catalog.tables[0]\n",
    "catalog_df = pd.DataFrame.spatial.from_layer(catalog.tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackoverflow.com/questions/56802797/digraph-parallel-ordering\n",
    "\n",
    "def topological_sort_grouped(G):\n",
    "    \"\"\"\n",
    "    Performs a topological sort where nodes entering the queue at the same time are stored in the same element. \n",
    "    \n",
    "    Arguments:\n",
    "        - G: <Networkx Directed Graph>\n",
    "    Returns:\n",
    "        - <iterator>\n",
    "        \n",
    "    Source: https://stackoverflow.com/questions/56802797/digraph-parallel-ordering\n",
    "    \"\"\"\n",
    "    indegree_map = {v: d for v, d in G.in_degree() if d > 0}\n",
    "    zero_indegree = [v for v, d in G.in_degree() if d == 0]\n",
    "    while zero_indegree:\n",
    "        yield zero_indegree\n",
    "        new_zero_indegree = []\n",
    "        for v in zero_indegree:\n",
    "            for _, child in G.edges(v):\n",
    "                indegree_map[child] -= 1\n",
    "                if not indegree_map[child]:\n",
    "                    new_zero_indegree.append(child)\n",
    "        zero_indegree = new_zero_indegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"\n",
    "    Divides a lst into chunks of size n.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
=======
   "execution_count": 75,
>>>>>>> main
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/arcgis/gis/__init__.py:676: UserWarning: You are logged on as cgs_transfer with an administrator role, proceed with caution.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def rebuild_relations(destination: GIS, relation_map: dict):\n",
    "    \"\"\"\n",
    "    Helper Function\n",
    "    \n",
    "    Applies origin item relations to the destination portal item.\n",
    "    \n",
    "    Arguments:\n",
    "        destination (arcgis.gis.GIS): Destination GIS for the origin item.\n",
    "        relation_map (dict): Relation map returned from wc_transfer() that maps the origin and destination IDs. \n",
    "    \"\"\"\n",
    "    \n",
    "    # traverse all keys in the relation map, representing the source item IDs\n",
    "    for key in relation_map.keys():\n",
    "        origin_item = origin_items_by_id[key]\n",
    "        destination_itemid = relation_map[key]\n",
    "        destination_item = destination.content.get(destination_itemid)\n",
    "        \n",
    "        # find the relations attatched to this ID \n",
    "        for relationship in RELATIONSHIP_TYPES:\n",
    "            try:\n",
    "                origin_related_items = origin_item.related_items(relationship)\n",
    "                \n",
    "                # apply relationships to target items \n",
    "                for origin_related_item in origin_related_items:\n",
    "                    destination_related_itemid = relation_map[origin_related_item.itemid]\n",
    "                    destination_related_item = destination.content.get(destination_related_itemid)\n",
    "                    status = destination_item.add_relationship(destination_related_item, relationship)\n",
    "                    print(f\"After execution, relation type {status} between {destination_item.title} and {destination_related_item.title} is {status}.\")\n",
    "            except Exception as rel_ex: # bare except clause temporary\n",
    "                print(f\"Error when checking for {relationship}: {rel_ex}\")\n",
    "                continue\n",
    "    \n",
    "    print(\"Process complete. Please check your content on the destination portal and review errors raised in this notebook.\")\n",
    "    \n",
    "def export_resources(item, save_path=None, file_name=None):\n",
    "    \"\"\"\n",
    "    Helper function, from https://developers.arcgis.com/python/samples/clone-storymap-version2/\n",
    "    Export's the data's resources as a zip file\n",
    "    \"\"\"\n",
    "    \n",
    "    url = f'{item._gis._portal.resturl}content/users/{item._user_id}/items/{item.itemid}/resources/export'\n",
    "    if save_path is None:\n",
    "        save_path = tempfile.gettempdir()\n",
    "    if file_name is None:\n",
    "        file_name = f\"{uuid.uuid4().hex[:6]}.zip\"\n",
    "    params = {'f' : 'zip'}\n",
    "    con = item._gis._portal.con\n",
    "    resources = con.get(url, params=params,\n",
    "                        out_folder=save_path,\n",
    "                        file_name=file_name,\n",
    "                        try_json=False)\n",
    "    return resources\n",
    "\n",
    "def get_layer_item_ids(wm):\n",
    "    \"\"\"\n",
    "    Helper function from https://developers.arcgis.com/python/guide/cloning-content/\n",
    "    \n",
    "    Returns the related items in a webmap.\n",
    "    \n",
    "    Params:\n",
    "        wm (argis.gis.Item): Webmap item to be inspected.\n",
    "    Returns:\n",
    "        wm_id_list (list): List of related items in the web map.\n",
    "    \"\"\"\n",
    "    wmo = WebMap(wm)\n",
    "    wm_id_list = []\n",
    "    \n",
    "    for layer in wmo.layers:\n",
    "        try:\n",
    "            fsvc = FeatureLayerCollection(layer['url'][:-1], origin)\n",
    "            if not fsvc.properties['serviceItemId'] in wm_id_list:\n",
    "                wm_id_list.append(fsvc.properties['serviceItemId'])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return wm_id_list\n",
    "\n",
    "def iterate_all(iterable, returned=\"key\"):\n",
    "    # Credits: https://gist.github.com/PatrikHlobil/9d045e43fe44df2d5fd8b570f9fd78cc\n",
    "    \n",
    "    \"\"\"Returns an iterator that returns all keys or values\n",
    "       of a (nested) iterable.\n",
    "       \n",
    "       Arguments:\n",
    "           - iterable: <list> or <dictionary>\n",
    "           - returned: <string> \"key\" or \"value\"\n",
    "           \n",
    "       Returns:\n",
    "           - <iterator>\n",
    "    \"\"\"\n",
    "  \n",
    "    if isinstance(iterable, dict):\n",
    "        for key, value in iterable.items():\n",
    "            if returned == \"key\":\n",
    "                yield key\n",
    "            elif returned == \"value\":\n",
    "                if not (isinstance(value, dict) or isinstance(value, list)):\n",
    "                    yield value\n",
    "            else:\n",
    "                raise ValueError(\"'returned' keyword only accepts 'key' or 'value'.\")\n",
    "            for ret in iterate_all(value, returned=returned):\n",
    "                yield ret\n",
    "    elif isinstance(iterable, list):\n",
    "        for el in iterable:\n",
    "            for ret in iterate_all(el, returned=returned):\n",
    "                yield ret\n",
    "                \n",
    "def find_relates(item: Item, gis=GIS(\"home\")):\n",
    "    \"\"\"\n",
    "    Finds the AGOL items nested inside an item without needing an Enterprise platform or \n",
    "    manually specifying relationships using the ArcGIS API. Does not tell you the type of \n",
    "    relationship between items, only the ItemIDs associated with a given item.\n",
    "    \n",
    "    Arguments:\n",
    "        gis (GIS): GIS object from arcgis.gis.GIS that the item lives in.\n",
    "        item (Item): Item object from arcgis.gis.Item\n",
    "        \n",
    "    Returns:\n",
    "        related_ids (set): All unique item IDs related to an item.\n",
    "    \"\"\"\n",
    "    \n",
    "    related_ids = []\n",
    "    item_json = item.get_data(try_json=True)\n",
    "    formatted_keys = iterate_all(item_json, returned=\"value\")\n",
    "    \n",
    "    json_vals = list(formatted_keys)\n",
    "    json_vals = [val for val in json_vals if isinstance(val, str)]\n",
    "    \n",
    "    for val in json_vals:\n",
    "        try:\n",
    "            search = gis.content.get(val)\n",
    "            if search != None:\n",
    "                related_ids.append(val)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return set(related_ids)\n",
    "\n",
    "def get_dash_wm(dash):\n",
    "    \"\"\"\n",
    "    From https://developers.arcgis.com/python/guide/cloning-content/#helper-functions\n",
    "    \n",
    "    Returns a list of all Web Maps participating in a Dashboard. \n",
    "    \n",
    "    Arguments:\n",
    "        dash (item): Dashboard to return participating Web Maps from.\n",
    "    Returns:\n",
    "        (list): All Web Maps partipating in the dashboard. \n",
    "    \n",
    "    \"\"\"\n",
    "    return [origin.content.get(widget['itemId']) \n",
    "            for widget in dash.get_data()[\"desktopView\"]['widgets']\n",
    "            if widget['type'] == \"mapWidget\"]\n",
    "\n",
    "def fixup(data, key, value, old_key):\n",
    "    if isinstance(data, dict):\n",
    "        return {\n",
    "            k: value if k == key and value == old_key else fixup(v, key, value, old_key) \n",
    "            for k, v in data.items()\n",
    "        }\n",
    "    elif isinstance(data, list):\n",
    "        return [fixup(v, key, value, old_key) for v in data]\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_transfer(destination: GIS, items=[], record=catalog):\n",
    "    \"\"\"\n",
    "    Performs a web content transfer of items from an origin to destination AGOL. \n",
    "    \n",
    "    Arguments:\n",
    "        destination (arcgis.gis.GIS): Destination GIS for the given items\n",
    "        items (list): a list of Items to be transferred.\n",
    "    \"\"\"\n",
    "    \n",
    "    record_table = record.tables[0]\n",
    "    record_df = pd.DataFrame.spatial.from_layer(record_table)\n",
    "    \n",
    "    origin_to_destination_ids = {}\n",
    "    \n",
    "    destination_items = destination.content.clone_items(items)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    tag = f\"src_{origin.properties['urlKey']}_{now.month}/{now.day}/{now.year}-{now.hour}:{now.minute}\"\n",
    "    \n",
    "    for item in destination_items:\n",
    "        item.update({'tags': tag})\n",
    "        \n",
    "    # build origin to destination map and memoize to catalog\n",
    "    origin_item_index = 0\n",
    "    for destination_item in destination_items:            \n",
    "        origin_to_destination_ids[item.id] = destination_item.id \n",
    "            \n",
    "        #new_record = pd.DataFrame({\"source_id\": [items[origin_item_index].id],\n",
    "        #                   \"destination_id\": destination_item.id,\n",
    "        #                   \"title\": destination_item.title,\n",
    "        #                   \"owner\": destination_item.owner})\n",
    "        #pd.concat([record_df, new_record])\n",
    "        #origin_item_index += 1\n",
    "    \n",
    "   # record_df.to_csv(\"catalog.csv\")\n",
    "    \n",
    "    return destination_items\n",
    "    \n",
    "def dash_transfer(destination: GIS, dash: Item, swizzle=False):\n",
    "    \"\"\"    \n",
    "    Performs a web content transfer of a dashboard to destination AGOL.\n",
    "    \n",
    "    Arguments:\n",
    "        destination (GIS): Destination GIS for the given Dashboard\n",
    "        dash (Item): A Dashboard item in the origin GIS\n",
    "        swizzle (bool): If True, enables JSON swizzling to map keys to values. Future proofing for ArcGIS API 2.2 release.\n",
    "    \"\"\"\n",
    "            \n",
    "    dash_elements = get_dash_wm(dash=dash)\n",
    "    wm_items = {} # origin to destination ids\n",
    "    \n",
    "    destination.content.create_folder(dash.title)\n",
    "    \n",
    "    for ele in dash_elements:\n",
    "        \n",
    "        # check if the item exists in catalog and use shortcut if so\n",
    "        if ele.id in catalog_df['source_id'].unique():\n",
    "            try:\n",
    "                ele_dest_id  = catalog_df.loc[catalog_df['source_id'] == ele.id, 'destination_id'].values[0]\n",
    "                wm_items[ele.id] = ele_dest_id\n",
    "                destination_item = destination.content.get(ele_dest_id)\n",
    "                destination_item.move(dash.title)\n",
    "            except IndexError:\n",
    "                continue\n",
    "                \n",
    "        # if the item participating in the dashboard has not yet been cloned: \n",
    "        else:\n",
    "            try:\n",
    "                print(f\"Transferring {ele.title} to destination org ... \")\n",
    "                wc = wc_transfer(destination=destination, items=[ele])\n",
    "                web_map_dest = [item.id for item in wc if item.type == \"Web Map\"]\n",
    "                print(web_map_dest)\n",
    "                wm_items[ele.id] = web_map_dest[0]\n",
    "            except IndexError:\n",
    "                print(f\"Item {ele.title} has already been transferred, applying destination-side edits ... \")\n",
    "                ele_from_search = destination.content.search(query=f\"typekeywords:source-{ele.id}\")[0]\n",
    "                wm_items[ele.id] = ele_from_search.id\n",
    "                ele_from_search.move(dash.title)\n",
    "    \n",
    "    print(wm_items)\n",
    "    dest_dash = destination.content.clone_items(items=[dash], item_mapping=wm_items, folder=dash.title)\n",
    "\n",
    "    now = datetime.now()\n",
    "    tag = f\"src_{origin.properties['urlKey']}_{now.month}/{now.day}/{now.year}-{now.hour}:{now.minute}\"\n",
    "\n",
    "    for dash in dest_dash:\n",
    "        dash.update({'tags': tag})\n",
    "    \n",
    "    if swizzle:\n",
    "        # Swizzle the old and new IDs\n",
    "        cloned_dash = dest_dash[0]\n",
    "        dash_json = cloned_dash.get_data()\n",
    "        dash_str = json.dumps(dash_json)\n",
    "        \n",
    "        for key, val in wm_items.items():\n",
    "            dash_str = dash_str.replace(key, val)\n",
    "        \n",
    "        updated_data = json.loads(dash_str)\n",
    "\n",
    "        cloned_dash.update(item_properties = {}, data = updated_data)\n",
    "\n",
    "    print(f\"Dashboard clone successful. Refresh your content page.\")\n",
    "\n",
    "def sm_transfer(destination: GIS, item: Item):\n",
    "    \"\"\"\n",
    "    Adapted code sample from https://developers.arcgis.com/python/samples/clone-storymap-version2/\n",
    "    \n",
    "    Transfer protocol for Story Maps and their web content items. Does not call copy_items() as protocol is different for this content.\n",
    "    \n",
    "    Arguments:\n",
    "        destination (arcgis.gis.GIS): Destination GIS for the origin item.\n",
    "    \"\"\"\n",
    "    \n",
    "    catalog = destination.content.get(CATALOG_ID)\n",
    "    catalog_table = catalog.tables[0]\n",
    "    catalog_df = pd.DataFrame.spatial.from_layer(catalog.tables[0])\n",
    "\n",
    "    if item.id in catalog_df['source_id'].unique():\n",
    "        print(f\"Story Map with ID {item.id} has already been transferred. Moving to next portal item.\")\n",
    "        pass\n",
    "    story_map = item\n",
    "\n",
    "    # check version to apply relevant protocol\n",
    "    if _version <= [1, 8, 2]:\n",
    "        resource = export_resources(item=story_map)\n",
    "    else:\n",
    "        resource = story_map.resources.export()\n",
    "\n",
    "    # get story map item data from json to store related maps\n",
    "    story_map_json = story_map.get_data(try_json=True)\n",
    "\n",
    "    web_maps = set([v['data']['itemId'] for k, v in story_map_json['resources'].items() \\\n",
    "            if v['type'].lower().find('webmap')>-1])\n",
    "    express_maps = set([v['data']['itemId'] for k, v in story_map_json['resources'].items() \\\n",
    "            if v['type'].lower().find('expressmap')>-1])\n",
    "\n",
    "\n",
    "    webmap_mapper = {} # keys are origin IDs, values are destination IDs\n",
    "    for wm in web_maps:\n",
    "        webmap_to_copy = origin.content.get(wm)\n",
    "\n",
    "        # check if item has been tranferred, if no then duplicate, if yes point to content that already exists\n",
    "        if webmap_to_copy.id in catalog_df['source_id'].unique():\n",
    "            webmap_destination_id = catalog_df.loc[catalog_df['source_id'] == webmap_to_copy.id, 'destination_id']\n",
    "            webmap_mapper[webmap_to_copy.id] = webmap_destination_id\n",
    "        else:\n",
    "            cloned_webmaps = destination.content.clone_items([webmap_to_copy])\n",
    "            webmap_mapper[webmap_to_copy.id] = [i for i in cloned_webmaps if i.type == 'Web Map'][0].id\n",
    "\n",
    "            # memoize tranfer to catalog\n",
    "            new_record = pd.DataFrame({\"source_id\": [webmap_to_copy.id],\n",
    "                                       \"destination_id\": [webmap_mapper[webmap_to_copy.id]], # gets destination ID from corresponding origin ID\n",
    "                                       \"title\": [webmap_to_copy.title],\n",
    "                                       \"owner\": [webmap_to_copy.owner]})\n",
    "            catalog_df = pd.concat([catalog_df, new_record])\n",
    "    \n",
    "    # export to csv in memory, and overwrite current catalog feature layer\n",
    "    catalog_df.to_csv('catalog.csv')\n",
    "    \n",
    "    collection = FeatureLayerCollection.fromitem(catalog)\n",
    "    collection.manager.overwrite('catalog.csv')\n",
    "\n",
    "    # remap the old itemid to the new one\n",
    "    story_map_text = json.dumps(story_map_json)\n",
    "\n",
    "    for key, val in webmap_mapper.items():\n",
    "        story_map_text = story_map_text.replace(key, val)\n",
    "\n",
    "    new_item = destination.content.add({'type' : story_map.type,\n",
    "                             'tags' : story_map.tags,\n",
    "                             'title' : story_map.title,\n",
    "                             'description' : story_map.description,\n",
    "                             'typeKeywords' : story_map.typeKeywords,\n",
    "                             'extent' : story_map.extent,\n",
    "                             'text' :story_map_text}\n",
    "                            )\n",
    "\n",
    "    # bring in the storymap resources exported to a zip archive earlier\n",
    "    new_item.resources.add(resource, archive=True)\n",
    "\n",
    "    # update the url\n",
    "    new_item.update({'url': story_map.url.replace(story_map.id, new_item.id)})\n",
    "    print(f\"StoryMap transfer complete. You can visit it at {new_item.homepage}\")\n",
    "\n",
    "def transfer():\n",
    "    \"\"\"\n",
    "    Runs the appropriate transfer function for each item the user has chosen from the select widget.\n",
    "    \n",
    "    Arguments:\n",
    "        items (iterable): An iterable of Item IDs. Default is the set generated from the selection box in the Content Migration Notebook.\n",
    "    \"\"\"\n",
    "    item_origin = origin.content.get(ITEM_ID)\n",
    "    \n",
    "    # reassign item to origin transfer user\n",
    "    if item_origin.owner != ORIGIN_TRANSFER_USER:\n",
    "        item_origin.reassign_to(ORIGIN_TRANSFER_USER)\n",
    "    \n",
    "    # decide on appropriate workflow for item\n",
    "    if item_origin.type == \"StoryMap\":\n",
    "        sm_transfer(destination=destination, item=item_origin)\n",
    "    elif item_origin.type == \"Dashboard\":\n",
    "        dash_transfer(destination=destination, dash=item_origin, swizzle=True)\n",
    "    else:\n",
    "        wc_transfer(destination=destination, items=[item_origin])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring Where is my driver? to destination org ... \n",
      "['01a51ec0a91348ed99ccd23c0597f2eb']\n",
      "{'8c489f1a44864dc0823f95d1e2d5fbe9': '01a51ec0a91348ed99ccd23c0597f2eb'}\n",
      "Dashboard clone successful. Refresh your content page.\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "dict_list = []\n",
    "\n",
    "total_items = len(my_items)\n",
    "\n",
    "for chunk in chunks(my_items, 100):\n",
    "    for item in my_items:\n",
    "        \n",
    "        # format item sharing data\n",
    "        item_share = \"\"\n",
    "        if item.shared_with['everyone']:\n",
    "            item_share += \"Shared with everyone\"\n",
    "        if item.shared_with['org']:\n",
    "            item_share += \"Shared with org\"\n",
    "\n",
    "        # delete protection string\n",
    "        if item.protect:\n",
    "            deletion_status = \"True\"\n",
    "        else:\n",
    "            deletion_status = \"False\"\n",
    "            \n",
    "        # check if its xferred\n",
    "        if item.id in catalog['source_id'].unique():\n",
    "            xfer = \"True\"\n",
    "        else:\n",
    "            xfer = \"False\"\n",
    "            \n",
    "        # last edit string\n",
    "        last_edit = datetime.fromtimestamp(item.modified/1000).__str__()\n",
    "        \n",
    "        # get related items (heavy resource cost)\n",
    "        related_ids = []\n",
    "        item_json = item.get_data(try_json=True)\n",
    "        flat_keys = flatten_data(item_json)\n",
    "\n",
    "        for k, v in flat_keys.items():\n",
    "            if \"itemId\" in k:\n",
    "                related_ids.append(v)\n",
    "                \n",
    "        for fldr in folders:\n",
    "            if fldr['id']==item.ownerFolder:\n",
    "                fldr_name =fldr['title']\n",
    "            else:\n",
    "                fldr_name = 'root'\n",
    "                    \n",
    "        related_ids = set(related_ids)\n",
    "        if len(related_ids) == 0:\n",
    "            related_ids = \"\"\n",
    "\n",
    "        # loop and generate these dictionaries, store in a list\n",
    "        d = {\"Item Name\": item.title, \"Author\": item.owner, \"Sharing\": item_share, \"Description\": item.description, \"Item ID\": item.id, \"Groups\": item.shared_with['groups'],\n",
    "                            \"Date Updated\": last_edit, \"URL\": item.homepage, \"Type\": item.type, \"Folder\": fldr_name, \"Tags\": item.tags, \"Categories\": item.categories, \n",
    "                            \"Content Status\": item.content_status, \"Related Items\": related_ids, \"Delete Protection\": deletion_status, \"Transfer Status\": xfer}\n",
    "\n",
    "        dict_list.append(d)\n",
    "        del d\n",
    "\n",
    "print(\"Report generated successfully.\")\n",
    "\n",
    "report = pd.DataFrame(dict_list)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Directed Graph of Item Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "report['Project ID'] = np.empty((len(report), 0)).tolist()\n",
    "graph_view = report[['Item ID', 'Related Items']]\n",
    "graph_view = graph_view.loc[graph_view['Related Items'] != '']\n",
    "\n",
    "graph_data = {}\n",
    "\n",
    "for item in range(len(graph_view)):\n",
    "    graph_data[graph_view.iloc[item]['Item ID']] = list(graph_view.iloc[item]['Related Items'])\n",
    "    \n",
    "user_graph = nx.DiGraph(graph_data)\n",
    "\n",
    "#print(\"Visual representation of user's item dependencies: \")\n",
    "#nx.draw_networkx(user_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = list(topological_sort_grouped(user_graph))[0]\n",
    "\n",
    "for root in roots:\n",
    "    x = str(uuid.uuid4())[:8]\n",
    "    # filter for root project\n",
    "    report.loc[report[\"Item ID\"] == root, \"Project ID\"] = x\n",
    "    relates = report.loc[report['Item ID'] == root][\"Related Items\"].values[0]\n",
    "    for item in relates:\n",
    "        # condition where project ids have already been added \n",
    "        report.loc[(report[\"Item ID\"] == item) & (report[\"Project ID\"].str.len() != 0), \"Project ID\"] = x + \", \"\n",
    "        # condition where a project id does not already exists\n",
    "        report.loc[(report[\"Item ID\"] == item) & (report[\"Project ID\"].str.len() == 0), \"Project ID\"] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Excel File and Apply Conditional Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "today_str = today.strftime(\"%b-%d-%Y\")\n",
    "\n",
    "report_name = f\"{ORG_USER}_contentReport_{today}.xlsx\"\n",
    "\n",
    "writer = pd.ExcelWriter(report_name, engine=\"xlsxwriter\")\n",
    "report.to_excel(writer, sheet_name=\"Report\", startrow=1, header=False)\n",
    "\n",
    "workbook = writer.book\n",
    "worksheet = writer.sheets['Report']\n",
    "\n",
    "# custom header for report following esri style guideline\n",
    "header_format = workbook.add_format({'bold': True,\n",
    "                                     'bottom': 2,\n",
    "                                     'bg_color': \"#C7EAFF\"})\n",
    "# format for edge items\n",
    "edge_format = workbook.add_format({'bg_color': \"#BDF2C4\"})\n",
    "\n",
    "# write the custom header\n",
    "for col_num, value in enumerate(report.columns.values):\n",
    "    worksheet.write(0, col_num + 1, value, header_format)\n",
    "\n",
    "\n",
    "# change the row background color if item is a 'edge' item, meaning it has no related items or dependencies downstream\n",
    "for row in range(len(report)):\n",
    "    if report.iloc[row]['Related Items'] == '':\n",
    "        worksheet.set_row(row + 1, cell_format=edge_format)\n",
    "        \n",
    "worksheet.autofit()\n",
    "    \n",
    "writer.save()"
=======
    "transfer()"
>>>>>>> main
   ]
  }
 ],
 "metadata": {
  "esriNotebookRuntime": {
   "notebookRuntimeName": "ArcGIS Notebook Python 3 Standard",
   "notebookRuntimeVersion": "8.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
